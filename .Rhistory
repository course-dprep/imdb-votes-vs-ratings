library(haven)
library(psych)
library(data.table)
# Remove respondents 201 to 250
dodge_sub <- dodge[-c(201:250), ]
# Keep only psychographic items (V2–V31)
psych_vars <- dodge_sub[, 2:31]
# Eigenvalues and variance explained
eigg <- eigen(cor(as.matrix(psych_vars)))
data.table(
Factor = 1:length(eigg$values),
Eigenvalue = eigg$values,
Percentage = (eigg$values / sum(eigg$values)) * 100,
Cumulative = cumsum((eigg$values / sum(eigg$values)) * 100)
)
### QUESTION 4
fa_model5 <- principal(psych_vars, nfactors = 5, rotate = "varimax")
fa_model5$uniqueness
1 - fa_model5$uniqueness   # communalities
fa_model5 <- principal(psych_vars, nfactors = 5, rotate = "varimax")
print(loadings(fa_model5), cutoff = 0.4)
### QUESTION 6
fa_model5 <- principal(psych_vars, nfactors = 5, rotate = "varimax", scores = TRUE)
dodge_sub <- cbind(dodge_sub, fa_model5$scores)
reg_model <- lm(V1__purchase_intentions ~ RC1 + RC2 + RC3 + RC4 + RC5, data = dodge_sub)
summary(reg_model)
View(dodge)
install.packages(c("haven", "psych", "GPArotation", "dplyr"))
library(haven)
library(dplyr)
# Load SPSS file
df <- read_sav("dodge.sav")
library(haven)
dodge <- read_sav("~/Documents/Master_Y1_B1/Intro to research in marketing/Week 6/dodge.sav")
View(dodge)
library(haven)
library(dplyr)
# Load SPSS file
library(haven)
dodge <- read_sav("~/Documents/Master_Y1_B1/Intro to research in marketing/Week 6/dodge.sav")
View(dodge)
# Exclude respondents 201 to 250
df_sub <- df %>% slice(-c(201:250))
psych_vars <- df_sub %>% select(starts_with("V")) %>% select(-V1_purchase_intention)
# Exclude respondents 201 to 250
df_sub <- df %>% slice(-c(201:250))
# Install required packages (run once)
# install.packages(c("haven", "psych", "GPArotation", "dplyr"))
library(haven)
library(psych)
library(GPArotation)
library(dplyr)
# 1. Load the data ----------------------------------------------------------
df <- read_sav("dodge.sav")
library(haven)
library(psych)
library(GPArotation)
library(dplyr)
# Remove respondents 201 to 250
df_sub <- df %>% slice(-c(201:250))
library(haven)
library(psych)
library(GPArotation)
library(dplyr)
# 2. Remove respondents 201–250
dodge_sub <- dodge %>% slice(-c(201:250))
# 3. Select only the 30 psychographic items (drop purchase intention)
psych_vars <- dodge_sub %>%
select(starts_with("V")) %>%
select(-V1_purchase_intention)
psych_vars <- dodge_sub %>%
select(starts_with("V")) %>%
select(-V1_purchase_intention)
library(haven)
library(psych)
library(GPArotation)
library(dplyr)
# 2. Remove respondents 201–250
dodge_sub <- dodge %>% slice(-c(201:250))
# 3. Select only the 30 psychographic items (drop purchase intention)
psych_vars <- dodge_sub %>%
select(starts_with("V")) %>%
select(-V1_purchase_intention)
View(dodge_sub)
View(dodge)
library(haven)
library(psych)
library(GPArotation)
library(dplyr)
# 2. Remove respondents 201–250
dodge_sub <- dodge %>% slice(-c(201:250))
# 3. Select the 30 psychographic variables (drop V1__purchase_intentions)
psych_vars <- dodge_sub %>%
select(
V2_very_good_physical_condition,
V3_dress_for_fashion_not_comfort,
V4_have_more_stylish_clothes_than_my_friends,
V5_want_to_look_a_little_different_from_others,
V6_life_is_too_short_not_to_take_some_gambles,
V7_not_concerned_about_the_ozone_layer,
V8_the_govt_is_doing_too_much_to_control_pollution,
V9_basically_society_today_is_fine,
V10_dont_have_time_to_volunteer_for_charities,
V11_family_is_not_too_heavily_in_debt_today,
V12_pay_cash_for_everything_I_buy,
V13_spend_for_today_and_let_tomorrow_bring_what_it_will,
V14_use_credit_cards_because_I_can_slowly_pay_off_bill,
V15_seldom_use_coupons_when_I_shop,
V16_interest_rates_are_low_enough_so_I_can_buy_what_I_want,
V17_have_more_self_confidence_than_most_of_my_friends,
V18_like_to_be_considered_a_leader,
V19_others_often_ask_me_for_help,
V20_children_are_the_most_important_thing_in_a_marriage,
V21_would_rather_spend_a_quiet_night_at_home_than_go_out,
V22_American_made_cars_cant_compare_with_foreign_made,
V23_govt_should_restrict_imports_from_Japan,
V24_Americans_should_always_try_to_buy_American_products,
V25_would_like_to_take_a_trip_around_the_world,
V26_want_to_do_something_different_with_my_life,
V27_usually_among_the_first_to_try_new_products,
V28_like_to_work_hard_and_play_hard,
V29_skeptical_predictions_are_usually_wrong,
V30_can_do_anything_I_set_my_mind_to,
V31_in_five_years_my_income_will_be_a_lot_higher
)
# 2. Remove respondents 201–250
dodge_sub <- dodge %>% slice(-c(201:250))
psych_vars <- dodge_sub %>%
select(
V2_very_good_physical_condition,
V3_dress_for_fashion_not_comfort,
V4_have_more_stylish_clothes_than_my_friends,
V5_want_to_look_a_little_different_from_others,
V6_life_is_too_short_not_to_take_some_gambles,
V7_not_concerned_about_the_ozone_layer,
V8_the_govt_is_doing_too_much_to_control_pollution,
V9_basically_society_today_is_fine,
V10_dont_have_time_to_volunteer_for_charities,
V11_family_is_not_too_heavily_in_debt_today,
V12_pay_cash_for_everything_I_buy,
V13_spend_for_today_and_let_tomorrow_bring_what_it_will,
V14_use_credit_cards_because_I_can_slowly_pay_off_bill,
V15_seldom_use_coupons_when_I_shop,
V16_interest_rates_are_low_enough_so_I_can_buy_what_I_want,
V17_have_more_self_confidence_than_most_of_my_friends,
V18_like_to_be_considered_a_leader,
V19_others_often_ask_me_for_help,
V20_children_are_the_most_important_thing_in_a_marriage,
V21_would_rather_spend_a_quiet_night_at_home_than_go_out,
V22_American_made_cars_cant_compare_with_foreign_made,
V23_govt_should_restrict_imports_from_Japan,
V24_Americans_should_always_try_to_buy_American_products,
V25_would_like_to_take_a_trip_around_the_world,
V26_want_to_do_something_different_with_my_life,
V27_usually_among_the_first_to_try_new_products,
V28_like_to_work_hard_and_play_hard,
V29_skeptical_predictions_are_usually_wrong,
V30_can_do_anything_I_set_my_mind_to,
V31_in_five_years_my_income_will_be_a_lot_higher
)
# Load libraries
library(haven)
library(psych)
library(Hmisc)
library(corrplot)
library(data.table)
### QUESTION 1
# Step 1: Load dataset
dodge <- read_sav("~/Documents/Master_Y1_B1/Intro to research in marketing/Week 6/dodge.sav")
View(dodge)
# Remove respondents 201 to 250
dodge_sub <- dodge[-c(201:250), ]
# Keep only psychographic items (V2–V31)
psych_vars <- dodge_sub[, 2:31]
# Eigenvalues and variance explained
eigg <- eigen(cor(as.matrix(psych_vars)))
data.table(
Factor = 1:length(eigg$values),
Eigenvalue = eigg$values,
Percentage = (eigg$values / sum(eigg$values)) * 100,
Cumulative = cumsum((eigg$values / sum(eigg$values)) * 100)
)
# Run the 5-factor solution with varimax rotation
fa_model5 <- principal(psych_vars, nfactors = 5, rotate = "varimax", scores = TRUE)
# View uniqueness values (variance not explained)
fa_model5$uniqueness["V6__life_is_too_short_not_to_take_some_gambles"]
# Calculate communality (variance explained by factors)
comm_v6 <- 1 - fa_model5$uniqueness["V6__life_is_too_short_not_to_take_some_gambles"]
# Convert to percentage
comm_v6 * 100
eigg <- eigen(cor(as.matrix(psych_vars)))
data.table(
Factor = 1:length(eigg$values),
Eigenvalue = eigg$values,
Percentage = (eigg$values / sum(eigg$values)) * 100,
Cumulative = cumsum((eigg$values / sum(eigg$values)) * 100)
)
library(haven)
stores <- read_sav("~/Documents/Master_Y1_B1/Intro to research in marketing/Week 7/stores.sav")
View(stores)
library(haven)
stores <- read_sav("~/Documents/Master_Y1_B1/Intro to research in marketing/Week 7/stores.sav")
View(stores)
summary(stores)
### check assumptions
# Multicollinearity:
library(corrplot)
corrplot(cor(stores[, 5:12]), method="number")
#Remove educhi (column 10):
stores <- stores[, -10]
### Hierarchical clustering
#Ward’s method:
stores_hc <- hclust(dist(stores[, 5:11], method="euclidean"), method="ward.D2")
#Agglomeration schedule:
data.frame(stores_hc$merge)
#Dendrogram:
install.packages("factoextra")
library(factoextra)
fviz_dend(stores_hc, k=2, horiz=TRUE) # k can change this into other numbers to highlight different clusters
library(haven)
stores <- read_sav("~/Documents/Master_Y1_B1/Intro to research in marketing/Week 7/stores.sav")
View(stores)
summary(stores)
### check assumptions
# Multicollinearity:
library(corrplot)
corrplot(cor(stores[, 5:12]), method="number")
#Remove educhi (column 10):
stores <- stores[, -10]
### Hierarchical clustering
#Ward’s method:
stores_hc <- hclust(dist(stores[, 5:11], method="euclidean"), method="ward.D2")
#Agglomeration schedule:
data.frame(stores_hc$merge)
#Dendrogram:
install.packages("factoextra")
library(factoextra)
fviz_dend(stores_hc, k=2, horiz=TRUE) # k can change this into other numbers to highlight different clusters
library(haven)
stores <- read_sav("~/Documents/Master_Y1_B1/Intro to research in marketing/Week 7/stores.sav")
View(stores)
summary(stores)
### check assumptions
# Multicollinearity:
library(corrplot)
corrplot(cor(stores[, 5:12]), method="number")
#Remove educhi (column 10):
stores <- stores[, -10]
### Hierarchical clustering
#Ward’s method:
stores_hc <- hclust(dist(stores[, 5:11], method="euclidean"), method="ward.D2")
#Agglomeration schedule:
data.frame(stores_hc$merge)
#Dendrogram:
library(factoextra)
fviz_dend(stores_hc, k=2, horiz=TRUE) # k can change this into other numbers to highlight different clusters
library(haven)
stores <- read_sav("~/Documents/Master_Y1_B1/Intro to research in marketing/Week 7/stores.sav")
View(stores)
summary(stores)
### check assumptions
# Multicollinearity:
library(corrplot)
corrplot(cor(stores[, 5:12]), method="number")
#Remove educhi (column 10):
stores <- stores[, -10]
### Hierarchical clustering
#Ward’s method:
stores_hc <- hclust(dist(stores[, 5:11], method="euclidean"), method="ward.D2")
#Agglomeration schedule:
data.frame(stores_hc$merge)
#Dendrogram:
library(factoextra)
fviz_dend(stores_hc, k=2, horiz=TRUE) # k can change this into other numbers to highlight different clusters
#Elbow plot:
fviz_nbclust(stores[, 5:11], FUNcluster=hcut, method="wss")
#Assign stores to clusters (here: 3 clusters):
stores <- cbind(stores, cluster_hc=cutree(stores_hc, k=3))
#Number of stores per cluster:
table(stores$cluster_hc)
### K-means clustering
#K-means with initial cluster centers=outcomes of Ward’s
#Compute centroids:
stores_centroids <- stores[, .(agehh =
mean(agehh),
twoworks = mean(twoworks),
homeowne = mean(homeowne),
hhsize = mean(hhsize),
educlow = mean(educlow),
urban = mean(urban),
incomehi = mean(incomehi)),
by=c("cluster_hc")]
stores_centroids <- stores[, .(agehh = mean(agehh),                               twoworks = mean(twoworks),                               homeowne = mean(homeowne),                               hhsize = mean(hhsize),                               educlow = mean(educlow),                               urban = mean(urban),                               incomehi = mean(incomehi)), by=c("cluster_hc")]
library(data.table)
setDT(stores)
stores_centroids <- stores[, .(
agehh     = mean(agehh),
twoworks  = mean(twoworks),
homeowne  = mean(homeowne),
hhsize    = mean(hhsize),
educlow   = mean(educlow),
urban     = mean(urban),
incomehi  = mean(incomehi)
), by = cluster_hc]
#Cluster:
set.seed(123)
stores_km <- kmeans(stores[,5:11], centers=stores_centroids[,-1], nstart=25) #because we used 3 clusters before, K-means generates a 3-cluster solution as well # for a random start, type in the number of clusters (e.g., centers=3)
#Assign stores to clusters (here: 3 clusters):
stores <- cbind(stores, cluster_km=stores_km$cluster)
#Number of stores per cluster:
stores_km$size
Group means:
### group means and ANOVA
#Group means:
stores_km$centers
### relationships
#Are these clusters related to store size? 🡪 cross-tabs
stores_ct <- table(stores$stsize, stores$cluster_km) # note: stsize in rows, cluster_km (“Chi-squared approximation may be incorrect” error due to small cells) in columns
stores_ct
summary(stores_ct)
#Relationship with store revenue? 🡪 ANOVA
# see previous video tutorial…
################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################
library(haven)
stores <- read_sav("~/Documents/Master_Y1_B1/Intro to research in marketing/Week 7/stores.sav")
View(stores)
summary(stores)
### check assumptions
# Multicollinearity:
library(corrplot)
corrplot(cor(stores[, 5:12]), method="number")
#Remove educhi (column 10):
stores <- stores[, -10]
### Hierarchical clustering
#Ward’s method:
stores_hc <- hclust(dist(stores[, 5:11], method="euclidean"), method="ward.D2")
#Agglomeration schedule:
data.frame(stores_hc$merge)
#Dendrogram:
library(factoextra)
fviz_dend(stores_hc, k=2, horiz=TRUE) # k can change this into other numbers to highlight different clusters
#Elbow plot:
fviz_nbclust(stores[, 5:11], FUNcluster=hcut, method="wss")
#Assign stores to clusters (here: 3 clusters):
stores <- cbind(stores, cluster_hc=cutree(stores_hc, k=3))
#Number of stores per cluster:
table(stores$cluster_hc)
### K-means clustering
#K-means with initial cluster centers=outcomes of Ward’s
#Compute centroids:
library(data.table)
setDT(stores)
stores_centroids <- stores[, .(
agehh     = mean(agehh),
twoworks  = mean(twoworks),
homeowne  = mean(homeowne),
hhsize    = mean(hhsize),
educlow   = mean(educlow),
urban     = mean(urban),
incomehi  = mean(incomehi)
), by = cluster_hc]
#Cluster:
set.seed(123)
stores_km <- kmeans(stores[,5:11], centers=stores_centroids[,-1], nstart=25) #because we used 3 clusters before, K-means generates a 3-cluster solution as well # for a random start, type in the number of clusters (e.g., centers=3)
#Assign stores to clusters (here: 3 clusters):
stores <- cbind(stores, cluster_km=stores_km$cluster)
#Number of stores per cluster:
stores_km$size
### group means and ANOVA
#Group means:
stores_km$centers
#ANOVA:
# see previous video tutorial…
### relationships
#Are these clusters related to store size? 🡪 cross-tabs
stores_ct <- table(stores$stsize, stores$cluster_km) # note: stsize in rows, cluster_km (“Chi-squared approximation may be incorrect” error due to small cells) in columns
stores_ct
summary(stores_ct)
#Relationship with store revenue? 🡪 ANOVA
# see previous video tutorial…
View(stores)
#create right dataset (subset of stores) exclude area==4 and missing area
stores_sub <- subset(stores, area != 4 & !is.na(area))
# Check how many are left
nrow(stores_sub)
View(stores_sub)
### QUESTION 1
# Ward’s method clustering
stores_hc <- hclust(dist(stores_sub[, c("agehh","twoworks","homeowne",
"hhsize","educlow","urban","incomehi")],
method="euclidean"),
method="ward.D2")
# Cut tree into 3 clusters
stores_sub$cluster_hc <- cutree(stores_hc, k=3)
# Number of stores per cluster
table(stores_sub$cluster_hc)
min(table(stores_sub$cluster_hc))
# Compute cluster centroids
stores_centroids <- stores_sub[, .(
agehh     = mean(agehh),
twoworks  = mean(twoworks),
homeowne  = mean(homeowne),
hhsize    = mean(hhsize),
educlow   = mean(educlow),
urban     = mean(urban),
incomehi  = mean(incomehi)
), by = cluster_hc]
stores_centroids
### QUESTION 2
library(data.table)
setDT(stores_sub)
# Compute cluster centroids
stores_centroids <- stores_sub[, .(
agehh     = mean(agehh),
twoworks  = mean(twoworks),
homeowne  = mean(homeowne),
hhsize    = mean(hhsize),
educlow   = mean(educlow),
urban     = mean(urban),
incomehi  = mean(incomehi)
), by = cluster_hc]
stores_centroids
# Run K-means with these centroids
set.seed(123)
stores_km <- kmeans(
stores_sub[, .(agehh, twoworks, homeowne, hhsize, educlow, urban, incomehi)],
centers = stores_centroids[,-1],  # drop cluster label column
nstart = 25
)
# Inspect results
stores_km$centers
round(stores_km$centers[1, "educlow"], 3)
### QUESTION 3
# Suppose we use the 5-factor solution scores
set.seed(123) # reproducibility
km <- kmeans(dodge_sub[, c("RC1","RC2","RC3","RC4","RC5")], centers = 3)  # or 4, depending on assignment
### QUESTION 3
# Run K-means clustering (example with 3 clusters, depends on your assignment)
set.seed(123)
km <- kmeans(stores_sub[, c("var1","var2","var3")], centers = 3)  # replace with the vars you clustered on
### QUESTION 3
# Add k-means cluster assignment as factor
stores_sub$cluster_km <- as.factor(stores_km$cluster)
anova_model <- aov(educlow ~ cluster_km, data = stores_sub)
summary(anova_model)
### QUESTION 4
anova_revenue <- aov(revenue ~ cluster_km + surface, data = stores_sub)
summary(anova_revenue)
# Load the data
library(haven)
stores <- read_sav("stores.sav")
# Subset: exclude area = 4 and missing area
stores_sub <- subset(stores, area != 4 & !is.na(area))
library(haven)
stores <- read_sav("~/Documents/Master_Y1_B1/Intro to research in marketing/Week 7/stores.sav")
View(stores)
# Load the data
library(haven)
stores <- read_sav("~/Documents/Master_Y1_B1/Intro to research in marketing/Week 7/stores.sav")
View(stores)
# Subset: exclude area = 4 and missing area
stores_sub <- subset(stores, area != 4 & !is.na(area))
# Check remaining rows
nrow(stores_sub)
# Hierarchical clustering with Ward's method
stores_hc <- hclust(
dist(stores_sub[, c("agehh","twoworks","homeowne","hhsize",
"educlow","urban","incomehi")],
method = "euclidean"),
method = "ward.D2"
)
# Cut tree into 3 clusters
stores_sub$cluster_hc <- cutree(stores_hc, k = 3)
# Count stores per cluster
table(stores_sub$cluster_hc)
min(table(stores_sub$cluster_hc))
### QUESTION 2
library(data.table)
setDT(stores_sub)
# Compute cluster centroids from hierarchical clusters
stores_centroids <- stores_sub[, .(
agehh     = mean(agehh),
twoworks  = mean(twoworks),
homeowne  = mean(homeowne),
hhsize    = mean(hhsize),
educlow   = mean(educlow),
urban     = mean(urban),
incomehi  = mean(incomehi)
), by = cluster_hc]
stores_centroids
set.seed(123)   # for reproducibility
stores_km <- kmeans(
stores_sub[, .(agehh, twoworks, homeowne, hhsize, educlow, urban, incomehi)],
centers = stores_centroids[, -1],   # drop cluster_hc column
nstart = 25
)
stores_km$centers
round(stores_km$centers[1, "educlow"], 3)
### QUESTION 3
# Add k-means cluster assignments to the dataset
stores_sub$cluster_km <- factor(stores_km$cluster)
anova_model <- aov(educlow ~ cluster_km, data = stores_sub)
summary(anova_model)
pval <- summary(anova_model)[[1]][["Pr(>F)"]][1]
round(pval, 3)
### QUESTION 4
stores_sub$cluster_km <- factor(stores_km$cluster)
stores_sub$cluster_km <- factor(stores_km$cluster)
anova_revenue <- aov(revenue ~ cluster_km + surface, data = stores_sub)
summary(anova_revenue)
pval <- summary(anova_revenue)[[1]][["Pr(>F)"]][1]
round(pval, 3)
