#Agglomeration schedule:
data.frame(stores_hc$merge)
#Dendrogram:
library(factoextra)
fviz_dend(stores_hc, k=2, horiz=TRUE) # k can change this into other numbers to highlight different clusters
#Elbow plot:
fviz_nbclust(stores[, 5:11], FUNcluster=hcut, method="wss")
#Assign stores to clusters (here: 3 clusters):
stores <- cbind(stores, cluster_hc=cutree(stores_hc, k=3))
#Number of stores per cluster:
table(stores$cluster_hc)
### K-means clustering
#K-means with initial cluster centers=outcomes of Wardâ€™s
#Compute centroids:
library(data.table)
setDT(stores)
stores_centroids <- stores[, .(
agehh     = mean(agehh),
twoworks  = mean(twoworks),
homeowne  = mean(homeowne),
hhsize    = mean(hhsize),
educlow   = mean(educlow),
urban     = mean(urban),
incomehi  = mean(incomehi)
), by = cluster_hc]
#Cluster:
set.seed(123)
stores_km <- kmeans(stores[,5:11], centers=stores_centroids[,-1], nstart=25) #because we used 3 clusters before, K-means generates a 3-cluster solution as well # for a random start, type in the number of clusters (e.g., centers=3)
#Assign stores to clusters (here: 3 clusters):
stores <- cbind(stores, cluster_km=stores_km$cluster)
#Number of stores per cluster:
stores_km$size
### group means and ANOVA
#Group means:
stores_km$centers
#ANOVA:
# see previous video tutorialâ€¦
### relationships
#Are these clusters related to store size? ðŸ¡ª cross-tabs
stores_ct <- table(stores$stsize, stores$cluster_km) # note: stsize in rows, cluster_km (â€œChi-squared approximation may be incorrectâ€ error due to small cells) in columns
stores_ct
summary(stores_ct)
#Relationship with store revenue? ðŸ¡ª ANOVA
# see previous video tutorialâ€¦
View(stores)
#create right dataset (subset of stores) exclude area==4 and missing area
stores_sub <- subset(stores, area != 4 & !is.na(area))
# Check how many are left
nrow(stores_sub)
View(stores_sub)
### QUESTION 1
# Wardâ€™s method clustering
stores_hc <- hclust(dist(stores_sub[, c("agehh","twoworks","homeowne",
"hhsize","educlow","urban","incomehi")],
method="euclidean"),
method="ward.D2")
# Cut tree into 3 clusters
stores_sub$cluster_hc <- cutree(stores_hc, k=3)
# Number of stores per cluster
table(stores_sub$cluster_hc)
min(table(stores_sub$cluster_hc))
# Compute cluster centroids
stores_centroids <- stores_sub[, .(
agehh     = mean(agehh),
twoworks  = mean(twoworks),
homeowne  = mean(homeowne),
hhsize    = mean(hhsize),
educlow   = mean(educlow),
urban     = mean(urban),
incomehi  = mean(incomehi)
), by = cluster_hc]
stores_centroids
### QUESTION 2
library(data.table)
setDT(stores_sub)
# Compute cluster centroids
stores_centroids <- stores_sub[, .(
agehh     = mean(agehh),
twoworks  = mean(twoworks),
homeowne  = mean(homeowne),
hhsize    = mean(hhsize),
educlow   = mean(educlow),
urban     = mean(urban),
incomehi  = mean(incomehi)
), by = cluster_hc]
stores_centroids
# Run K-means with these centroids
set.seed(123)
stores_km <- kmeans(
stores_sub[, .(agehh, twoworks, homeowne, hhsize, educlow, urban, incomehi)],
centers = stores_centroids[,-1],  # drop cluster label column
nstart = 25
)
# Inspect results
stores_km$centers
round(stores_km$centers[1, "educlow"], 3)
### QUESTION 3
# Suppose we use the 5-factor solution scores
set.seed(123) # reproducibility
km <- kmeans(dodge_sub[, c("RC1","RC2","RC3","RC4","RC5")], centers = 3)  # or 4, depending on assignment
### QUESTION 3
# Run K-means clustering (example with 3 clusters, depends on your assignment)
set.seed(123)
km <- kmeans(stores_sub[, c("var1","var2","var3")], centers = 3)  # replace with the vars you clustered on
### QUESTION 3
# Add k-means cluster assignment as factor
stores_sub$cluster_km <- as.factor(stores_km$cluster)
anova_model <- aov(educlow ~ cluster_km, data = stores_sub)
summary(anova_model)
### QUESTION 4
anova_revenue <- aov(revenue ~ cluster_km + surface, data = stores_sub)
summary(anova_revenue)
# Load the data
library(haven)
stores <- read_sav("stores.sav")
# Subset: exclude area = 4 and missing area
stores_sub <- subset(stores, area != 4 & !is.na(area))
library(haven)
stores <- read_sav("~/Documents/Master_Y1_B1/Intro to research in marketing/Week 7/stores.sav")
View(stores)
# Load the data
library(haven)
stores <- read_sav("~/Documents/Master_Y1_B1/Intro to research in marketing/Week 7/stores.sav")
View(stores)
# Subset: exclude area = 4 and missing area
stores_sub <- subset(stores, area != 4 & !is.na(area))
# Check remaining rows
nrow(stores_sub)
# Hierarchical clustering with Ward's method
stores_hc <- hclust(
dist(stores_sub[, c("agehh","twoworks","homeowne","hhsize",
"educlow","urban","incomehi")],
method = "euclidean"),
method = "ward.D2"
)
# Cut tree into 3 clusters
stores_sub$cluster_hc <- cutree(stores_hc, k = 3)
# Count stores per cluster
table(stores_sub$cluster_hc)
min(table(stores_sub$cluster_hc))
### QUESTION 2
library(data.table)
setDT(stores_sub)
# Compute cluster centroids from hierarchical clusters
stores_centroids <- stores_sub[, .(
agehh     = mean(agehh),
twoworks  = mean(twoworks),
homeowne  = mean(homeowne),
hhsize    = mean(hhsize),
educlow   = mean(educlow),
urban     = mean(urban),
incomehi  = mean(incomehi)
), by = cluster_hc]
stores_centroids
set.seed(123)   # for reproducibility
stores_km <- kmeans(
stores_sub[, .(agehh, twoworks, homeowne, hhsize, educlow, urban, incomehi)],
centers = stores_centroids[, -1],   # drop cluster_hc column
nstart = 25
)
stores_km$centers
round(stores_km$centers[1, "educlow"], 3)
### QUESTION 3
# Add k-means cluster assignments to the dataset
stores_sub$cluster_km <- factor(stores_km$cluster)
anova_model <- aov(educlow ~ cluster_km, data = stores_sub)
summary(anova_model)
pval <- summary(anova_model)[[1]][["Pr(>F)"]][1]
round(pval, 3)
### QUESTION 4
stores_sub$cluster_km <- factor(stores_km$cluster)
stores_sub$cluster_km <- factor(stores_km$cluster)
anova_revenue <- aov(revenue ~ cluster_km + surface, data = stores_sub)
summary(anova_revenue)
pval <- summary(anova_revenue)[[1]][["Pr(>F)"]][1]
round(pval, 3)
knitr::opts_chunk$set(warning = FALSE, message = FALSE)
### Setup ###
# Loading Libraries
source("loading-packages.R")
knitr::opts_chunk$set(warning = FALSE, message = FALSE)
### Setup ###
# Loading Libraries
source("../1-raw-data/loading-packages.R")
---
title: "Votes vs Valence: exploring the quadratic relationship between ratings and vote counts on IMDb"
knitr::opts_chunk$set(warning = FALSE, message = FALSE, fig.align = "center", out.width = "100%")
# Use project-rooted paths everywhere
if (!requireNamespace("here", quietly = TRUE)) install.packages("here")
library(here)
# (optional but nice) anchor the project once by creating an empty `.here` file in the repo root
# Load your packages via project-rooted path
source(here("src", "1-raw-data", "loading-packages.R"))
```
```{r setup, include=FALSE}
```{r}
knitr::opts_chunk$set(warning = FALSE, message = FALSE, fig.align = "center", out.width = "100%")
if (!requireNamespace("here", quietly = TRUE)) install.packages("here")
library(here)
source(here("src", "1-raw-data", "loading-packages.R"))
knitr::include_graphics(here("gen", "output", "model4.png"))
knitr::include_graphics(here("gen", "output", "regression_models.png"))
#Load required packages
source("../1-raw-data/loading-packages.R")
#Load required packages
source("../1-raw-data/loading-packages.R")
print('starting download')
# Load required packages (subfolder-relative; we run from src/1-raw-data/)
source("loading-packages.R")
dir.create(data_dir, recursive = TRUE, showWarnings = FALSE)
print('starting download')
# Load required packages (subfolder-relative; we run from src/1-raw-data/)
source("loading-packages.R")
print('starting download')
# Load required packages (subfolder-relative; we run from src/1-raw-data/)
source("loading-packages.R")
# Data directory is the sibling folder ../../data
data_dir <- "../../data"
dir.create(data_dir, recursive = TRUE, showWarnings = FALSE)
basics_path  <- file.path(data_dir, "title.basics.tsv.gz")
ratings_path <- file.path(data_dir, "title.ratings.tsv.gz")
print('starting download')
# Load required packages (subfolder-relative; we run from src/1-raw-data/)
source("../1-raw-data/loading-packages.R")
source("../1-raw-data/loading-packages.R")
source("../1-raw-data/loading-packages.R")
source("../1-raw-data/loading-packages.R")
if (!requireNamespace("here", quietly = TRUE)) install.packages("here")
library(here)
source(here("src", "1-raw-data", "loading-packages.R"))
source("../1-raw-data/loading-packages.R")
print('starting download')
# Load required packages
if (!requireNamespace("here", quietly = TRUE)) install.packages("here")
library(here)
source(here("src", "1-raw-data", "loading-packages.R"))
# Data directory is the sibling folder ../../data
data_dir <- "../../data"
dir.create(data_dir, recursive = TRUE, showWarnings = FALSE)
basics_path  <- file.path(data_dir, "title.basics.tsv.gz")
ratings_path <- file.path(data_dir, "title.ratings.tsv.gz")
#Define source Urls (IMDb datasets)
"title.basics: contains title-level information (e.g., type, year, genres)"
"title.ratings: contains IMDb user ratings and vote counts."
basics_url  <- "https://datasets.imdbws.com/title.basics.tsv.gz"
ratings_url <- "https://datasets.imdbws.com/title.ratings.tsv.gz"
#Download datasets to local "data" directory
download.file(basics_url, basics_path, mode = "wb", quiet = TRUE)
download.file(ratings_url, ratings_path, mode = "wb", quiet = TRUE)
#Create a random sample with reproducibility
"Seed set at 123 ensures the same data used if the analysis is re-run. Set n = 200,000 to limit computation power required for the analysis."
set.seed(123)
sample_basics <- vroom(basics_path, delim = "\t") %>%
slice_sample(n = 200000)
set.seed(123)
sample_ratings <- vroom(ratings_path, delim = "\t") %>%
slice_sample(n = 200000)
# Output
message("Random samples created:")
message("Sample basics rows: ", nrow(sample_basics))
message("Sample ratings rows: ", nrow(sample_ratings))
print('download complete')
print('start importing data for clean')
# Load packages (subfolder-relative)
if (!requireNamespace("here", quietly = TRUE)) install.packages("here")
library(here)
source(here("src", "1-raw-data", "loading-packages.R"))
# READ RAW DATA (from root-level data folder)
basics  <- read_tsv("../../data/title.basics.tsv.gz",  na = "\\N",
col_select = c(tconst, titleType, startYear, genres),
show_col_types = FALSE)
ratings <- read_tsv("../../data/title.ratings.tsv.gz", na = "\\N",
col_select = c(tconst, averageRating, numVotes),
show_col_types = FALSE)
print('imported data for clean')
# TRANSFORMATION
#Merge the datasets based on the common identifier (tconst)
imdb_raw <- basics %>%
inner_join(ratings, by = "tconst")
#Type casting and selection of relevant title types
# - Convert startYear into integer
# - Classify titles into simplified categories: movie, series, other
# - Filter to keep only movies and series
imdb_raw <- imdb_raw %>%
mutate(
startYear = suppressWarnings(as.integer(startYear)),
type = if_else(titleType %in% c("movie", "tvMovie"), "movie",
if_else(titleType %in% c("tvSeries", "tvMiniSeries"), "series", "other"))
) %>%
filter(type %in% c("movie", "series"))
#Remove duplicates based on the identifier - i.e., if any titletype appears more than once in the dataset.
imdb_dedup <- imdb_raw %>%
distinct(tconst, .keep_all = TRUE)
#Filter out titles with very few votes to reduce noise
"Inspect numVotes variable; median is 49, mean is 2746 votes. This means that the numVotes is heavlity skewed to the right; ie, many titles have just a few votes. Although number of votes (numVotes) presents the variable of interest of our research question, we have decided to only keep titles which have at least 20 votes. The reason is to reduce noise: titles with just one or two reviews ma have ratings which are not representative for the movie, whilst ratings by several people may reduce this noise whilst keeping more extreme opinions of a small group."
imdb_dedup %>%
summarise(
min_votes = min(numVotes, na.rm = TRUE),
max_votes = max(numVotes, na.rm = TRUE),
median_votes = median(numVotes, na.rm = TRUE),
mean_votes = mean(numVotes, na.rm = TRUE),
sd_votes = sd(numVotes, na.rm = TRUE)
)
imdb_clean <- imdb_dedup %>%
filter(numVotes >= 20)
# Map individual genres into broader genre families
# 1.Remove rows with missing or empty genres. Note that this is necessary because data-filing processes do not make sense here; one cannot infer the genre of a title based on the genre of the title that was realized in the same year (or based on another variable).
# 2.Split multiple genres into separate rows
# 3.Assign each genre to a family: 'Escapist'(fantasy, romance, action, adventure, animation, family) or 'Heavy' (drama, thriller, biography, crime, documentary and heavy) or mixed (both escapist and heavy)
# 4.Summarize back to one row per title, combining genre families
genre_map <- imdb_clean %>%
filter(!is.na(genres), genres != "") %>%
select(tconst, genres) %>%
separate_rows(genres, sep = ",") %>%
mutate(fam = case_when(
genres %in% c("Fantasy","Comedy","Romance","Action","Adventure","Animation","Family") ~ "Escapist",
genres %in% c("Drama","Thriller","Biography","Crime","Documentary") ~ "Heavy",
TRUE ~ NA_character_
)) %>%
group_by(tconst) %>%
summarise(
genre_family = case_when(
all(is.na(fam))               ~ NA_character_,
n_distinct(na.omit(fam)) == 1 ~ first(na.omit(fam)),
n_distinct(na.omit(fam)) >  1 ~ "Mixed"
),
.groups = "drop"
)
#Merge new genre categories back into dataset
"Ensure the imdb_clean does not have empty genre rows either; enrich with extra genre features"
imdb_clean <- imdb_clean %>%
filter(!is.na(genres), genres != "")
imdb_enriched <- imdb_clean %>%
left_join(genre_map, by = "tconst") %>%
select(tconst, titleType, type, startYear, genres, genre_family,
averageRating, numVotes)
# OUTPUT (to root-level data/clean)
dir.create("../../data/clean", showWarnings = FALSE, recursive = TRUE)
write_csv(imdb_clean,    "../../data/clean/imdb_clean.csv")
write_rds(imdb_clean,    "../../data/clean/imdb_clean.rds")
write_csv(imdb_enriched, "../../data/clean/imdb_enriched.csv")
write_rds(imdb_enriched, "../../data/clean/imdb_enriched.rds")
print('output added to data/clean')
#Merge new genre categories back into dataset
"Ensure the imdb_clean does not have empty genre rows either; enrich with extra genre features"
imdb_clean <- imdb_clean %>%
filter(!is.na(genres), genres != "")
imdb_enriched <- imdb_clean %>%
left_join(genre_map, by = "tconst") %>%
select(tconst, titleType, type, startYear, genres, genre_family,
averageRating, numVotes)
# OUTPUT (to root-level data/clean)
dir.create("../../data/clean", showWarnings = FALSE, recursive = TRUE)
write_csv(imdb_clean,    "../../data/clean/imdb_clean.csv")
write_rds(imdb_clean,    "../../data/clean/imdb_clean.rds")
write_csv(imdb_enriched, "../../data/clean/imdb_enriched.csv")
write_rds(imdb_enriched, "../../data/clean/imdb_enriched.rds")
print('output added to data/clean')
#Load required packages
if (!requireNamespace("here", quietly = TRUE)) install.packages("here")
library(here)
source(here("src", "1-raw-data", "loading-packages.R"))
#Load imdb dataset
imdb_analysis_main <- read_csv("../../data/clean/imdb_analysis.csv")
#Load required packages
if (!requireNamespace("here", quietly = TRUE)) install.packages("here")
library(here)
source(here("src", "1-raw-data", "loading-packages.R"))
#Load imdb dataset
imdb_analysis_main <- read_csv("../../data/clean/imdb_analysis.csv")
print('start load dataset prep')
# Load required packages
source("../1-raw-data/loading-packages.R")
print('start load dataset prep')
# Load required packages
if (!requireNamespace("here", quietly = TRUE)) install.packages("here")
library(here)
source(here("src", "1-raw-data", "loading-packages.R"))
imdb_analysis <- read_csv("../../data/clean/imdb_enriched.csv")
print('dataset loaded')
imdb_analysis <- imdb_analysis %>%
mutate(votes2 = numVotes^2)
#Log transform to deal with skewed data
imdb_analysis <- imdb_analysis %>%
mutate(
log_votes = log1p(numVotes),
log_votes2 = log_votes^2)
"Create a variable of time periods with each time period being of approximately equal length. In this way, we can control whether in certain periods certain genres were more or less popular for example"
imdb_analysis <- imdb_analysis %>%
mutate(period = case_when(
startYear <= 1925 ~ "Pre-War",
startYear <= 1960 ~ "Interwar",
startYear <= 1995 ~ "Post-War",
startYear >  1995 ~ "Modern",
TRUE ~ NA_character_
))
imdb_analysis <- imdb_analysis %>%
mutate(rating_category = cut(
averageRating,
breaks = seq(0, 10, by = 2),
labels = c("Very Bad", "Bad", "Average", "Good", "Excellent"),
include.lowest = TRUE,
right = TRUE))
imdb_analysis <- imdb_analysis %>%
mutate(rating_category = factor(rating_category, levels = c("Very Bad", "Bad", "Average", "Good", "Excellent"), ordered = TRUE))
# Output
write_csv(imdb_analysis, "../../data/clean/imdb_analysis.csv")
write_rds(imdb_analysis, "../../data/clean/imdb_analysis.rds")
print('output prep added')
if (!requireNamespace("here", quietly = TRUE)) install.packages("here")
library(here)
source(here("src", "1-raw-data", "loading-packages.R"))
#Load imdb dataset
imdb_analysis_main <- read_csv("../../data/clean/imdb_analysis.csv")
#Keep only those observations with an exclusive genre
imdb_analysis_main <- imdb_analysis_main %>%
filter(genre_family == "Escapist" | genre_family == "Heavy")
model_linear <- lm(averageRating ~ log_votes + period, data = imdb_analysis_main)
summary(model_linear)
tidy(model_linear)
print('Model 1 made')
model_quadratic <- lm(averageRating ~ log_votes + log_votes2 + period, data = imdb_analysis_main)
summary(model_quadratic)
tidy(model_quadratic)
print('Model 2 made')
anova(model_linear, model_quadratic)
model_interaction_genre <- lm( averageRating ~ log_votes + log_votes2 + genre_family +
log_votes*genre_family + log_votes2*genre_family, data = imdb_analysis_main)
summary(model_interaction_genre)
tidy(model_interaction_genre)
print('Model 3 made')
model_interaction_type <- lm(
averageRating ~ log_votes + log_votes2 + type +
log_votes*type + log_votes2*type,
data = imdb_analysis_main)
print('Model 4 made')
summary(model_interaction_type)
tidy(model_interaction_type)
#Visualization
#Model 1 & 2
Model_1_2 <- ggplot(imdb_analysis_main, aes(x = log_votes, y = averageRating)) +
geom_point(alpha = 0.1) +   # raw data
geom_smooth(method = "lm", formula = y ~ x, color = "blue") +   # linear
geom_smooth(method = "lm", formula = y ~ poly(x, 2), color = "violet") +   # quadratic
labs(title = "Average Rating vs Number of votes, linear and quadratic log number of votes",
x = "Log(Number of Votes)", y = "Average Rating")
print('Model 1,2 done')
Model_3 <- ggplot(imdb_analysis_main, aes(x = log_votes, y = averageRating)) +
geom_point(alpha = 0.1, color = "gray50") +
geom_smooth(aes(color = genre_family),
method = "lm", formula = y ~ poly(x, 2), se = FALSE, size = 1) +
labs(title = "Rating vs Votes (logged) by Genre (Heavy versus Escapist)",
x = "Log(Number of Votes)", y = "Average Rating",
color = "Genre") +
facet_wrap(~ genre_family) +
theme_minimal()
print('Model 3 done')
Model_4 <- ggplot(imdb_analysis_main, aes(x = log_votes, y = averageRating)) +
geom_point(alpha = 0.1, color = "gray50") +
geom_smooth(aes(color = type),
method = "lm", formula = y ~ poly(x, 2), se = FALSE, size = 1) +
labs(title = "Rating vs Votes (logged) by Content Type (Movie vs Series)",
x = "Log(Number of Votes)", y = "Average Rating",
color = "Content Type") +
facet_wrap(~ type) +
theme_minimal()
print('Model 4 done')
print('Visualization of all models done')
dir.create("../../gen/output", recursive = TRUE, showWarnings = FALSE)
ggsave("../../gen/output/model1_2.png", Model_1_2, width = 8, height = 6)
ggsave("../../gen/output/model3.png",   Model_3,   width = 8, height = 6)
ggsave("../../gen/output/model4.png",   Model_4,   width = 8, height = 6)
print('models saved in gen/output')
install.packages("modelsummary")
install.packages("modelsummary")
# Load
library(modelsummary)
models <- list(
"Linear" = model_linear,
"Quadratic" = model_quadratic,
"Genre Interaction" = model_interaction_genre,
"Type Interaction" = model_interaction_type
)
modelsummary(
models,
output = "../../gen/output/regression_models.png",
title = "Regression Models: Ratings vs Votes",
stars = TRUE
)
install.packages('webshot2')
library(webshot2)
modelsummary(
models,
output = "../../gen/output/regression_models.png",
title = "Regression Models: Ratings vs Votes",
stars = TRUE
)
dir.create(here("gen","output"), recursive = TRUE, showWarnings = FALSE)
html_path <- here("gen","output","regression_models.html")
png_path  <- here("gen","output","regression_models.png")
modelsummary(
models,
output = html_path,
title  = "Regression Models: Ratings vs Votes",
stars  = TRUE
)
# turn the HTML into a PNG
if (!requireNamespace("webshot2", quietly = TRUE)) install.packages("webshot2")
webshot2::webshot(html_path, file = png_path, vwidth = 1600, zoom = 1.5)
fig-regression, echo=FALSE, message=FALSE, warning=FALSE,
{fig-regression, echo=FALSE, message=FALSE, warning=FALSE,
```{r fig-regression, echo=FALSE, message=FALSE, warning=FALSE,
